{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar of personhood (machine learning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "I'm working on my personification chapter now. So far I've measured an [\"agency index\"](https://twitter.com/quadrismegistus/status/1059305496211931136), trying to find trends in how words gain/lose (syntactic) \"agency\" over time. This has been interesting, but what's missing is that personifications don't just do things (as the agency index captures), they do *human* things (\"let not Ambition *mock*\") and have *human* things (\"Honour's voice\").\n",
    "\n",
    "So I'm trying to move on from this 'syntax of agency' to a broader 'grammar of personhood'. I took a failed stopover at the 'semantics of personhood' via word2vec: my 'human words' vector didn't end up being that interesting an index for other words, i.e. didn't seem to capture personification effects. Now I'm moving back to the syntactic BookNLP-style data (subject-verb, modifier-noun, etc) I've collected about the nouns for the Chadwyck Healey poetry collections.\n",
    "\n",
    "I'm wondering whether a classifier to separate human vs. non-human (maybe human vs. object) words by way of the distribution of other words ('collocated' by syntax): and then use that classifier to estimate the 'humanness' of all words, not just those in the cross-validation experiment? I did this in a smaller related project on animal stories, and according to the cross-validation results, the model found it easier to separate humans and animals in novels than it did in these anthropomorphic animal stories, which seemed right. But here I want to use a classifier to estimate the humanness of all, even non-human/object words like abstract nouns, to see if that changes over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide an initial set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> streaming as tsv: data.worddb.txt\n",
      "   done [0.5 seconds]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000,\n",
       " [u'fawn',\n",
       "  u'nunnery',\n",
       "  u'woods',\n",
       "  u'spiders',\n",
       "  u'hanging',\n",
       "  u'woody',\n",
       "  u'disobeying',\n",
       "  u'canes',\n",
       "  u'scold',\n",
       "  u'originality'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load words from the project-wide 25K\n",
    "from lit.tools import read_ld\n",
    "all_words = {d['word'] for d in read_ld('data.worddb.txt')}\n",
    "len(all_words),list(all_words)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform slingshot results into booknlp-like data\n",
    "Code adapted from the classification work in the [Wild Animal Stories notebook](http://localhost:8888/lab/tree/workspace%2Fwildanimalstories%2Fexperiments.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,pandas as pd,numpy as np,itertools\n",
    "from lit import tools\n",
    "rels = {\n",
    "        'poss':'Possessive',\n",
    "        'nsubj':'Subject',\n",
    "        'nsubjpass':'Subject (passive)',\n",
    "        'dobj':'Object',\n",
    "        'amod':'Modifier',\n",
    "        'compound':'Modifier',\n",
    "        'appos':'Modifier',\n",
    "        'attr':'Modifier',\n",
    "        'dative':'Object'\n",
    "       }\n",
    "\n",
    "PATH_TO_SLINGSHOT_RESULT_DATA = '../syntax/results_slingshot/spacy_syntax/parse_path2/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_results(fn=PATH_TO_SLINGSHOT_RESULT_DATA):\n",
    "    import pandas as pd,os\n",
    "    from mpi_slingshot import stream_results\n",
    "    for path,data in stream_results(fn):\n",
    "        if '.ipynb' in path: continue\n",
    "        sent_ld=[]\n",
    "        num_sent=0\n",
    "        fn=os.path.split(path)[-1]\n",
    "        for dx in data:\n",
    "            if sent_ld and dx['sent_start']!=sent_ld[-1]['sent_start']:\n",
    "                old=get_booknlp_like_data(sent_ld)\n",
    "                num_sent+=1\n",
    "                for odx in old:\n",
    "                    odx['num_sent']=num_sent\n",
    "                    odx['fn']=fn\n",
    "                    yield odx\n",
    "                    sent_ld=[]\n",
    "            sent_ld+=[dx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_booknlp_like_data(sent_ld,pos_only={'NOUN'},lemma=False):\n",
    "    \"\"\"\n",
    "    Modifiers\n",
    "    Nouns possessed by characters: poss\n",
    "    Adjectives modifying characters: \n",
    "    Verbs of which character is a subject\n",
    "    Verbs of which character is an object\n",
    "    \n",
    "    rels = {'poss':'Possessive',\n",
    "           'nsubj':'Subject',\n",
    "           'dobj':'Object',\n",
    "           'amod':'Modifier'}\n",
    "    \"\"\"\n",
    "    \n",
    "    old=[]\n",
    "    for dx in sent_ld:\n",
    "        word=dx['lemma'] if lemma else dx['word']\n",
    "        rel=dx['dep']\n",
    "        head=dx['head_lemma'] if lemma else dx['head']\n",
    "        pos=dx['pos']\n",
    "        word,head=word.lower(),head.lower()\n",
    "        if not word in all_words or not pos in pos_only: continue\n",
    "        word_dx={'head':head,'word':word,'rel':rel}\n",
    "        old+=[word_dx]\n",
    "    return old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ran this on Sherlock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterator\n",
    "transformer = transform_results(PATH_TO_SLINGSHOT_RESULT_DATA)\n",
    "#pd.DataFrame(list(itertools.islice(transformer,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools.writegen('./data.booknlp_like_data.chadwyck_poetry.txt', transform_results)\n",
    "# last run: 2/3/2019 13:49 PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloaded this data (*data.booknlp_like_data.chadwyck_poetry.txt.gz*) to *data_booknlp/*\n",
    "\n",
    "Data appears as:\n",
    "\n",
    "| fn             | head     | num_sent | rel      | word      |\n",
    "|----------------|----------|----------|----------|-----------|\n",
    "| Z400605772.xml | are      | 1        | nsubj    | hills     |\n",
    "| Z400605772.xml | knows    | 2        | dobj     | roads     |\n",
    "| Z400605772.xml | knows    | 2        | conj     | moves     |\n",
    "| Z400605772.xml | in       | 3        | pobj     | circles   |\n",
    "| Z400605772.xml | within   | 3        | pobj     | head      |\n",
    "| Z400605772.xml | has      | 4        | dobj     | say       |\n",
    "| Z400605772.xml | is       | 5        | nsubj    | river     |\n",
    "| Z400605772.xml | lie      | 5        | nsubj    | winds     |\n",
    "| Z400605772.xml | at       | 6        | pobj     | dawn      |\n",
    "| Z400605772.xml | sees     | 6        | dobj     | skies     |\n",
    "| Z400605772.xml | feels    | 7        | nsubj    | shadows   |\n",
    "| Z400605772.xml | of       | 7        | pobj     | night     |\n",
    "| Z400605772.xml | recline  | 7        | dobj     | fingers   |\n",
    "| Z400605772.xml | on       | 7        | pobj     | eyes      |\n",
    "| Z400605772.xml | welcomes | 8        | dobj     | sun       |\n",
    "| Z400605772.xml | sun      | 8        | conj     | rain      |\n",
    "| Z400605772.xml | has      | 9        | nsubj    | landscape |\n",
    "| Z400605772.xml | has      | 9        | dobj     | depth     |\n",
    "| Z400605772.xml | depth    | 9        | conj     | height    |\n",
    "| Z400605772.xml | city     | 10       | ROOT     | city      |\n",
    "| Z400605772.xml | burns    | 10       | compound | passion   |\n",
    "| Z400605772.xml | like     | 10       | pobj     | burns     |\n",
    "| Z400605772.xml | walks    | 11       | compound | morning   |\n",
    "| Z400605772.xml | of       | 11       | pobj     | walks     |\n",
    "| Z400605772.xml | on       | 11       | pobj     | wave      |\n",
    "| Z400605772.xml | of       | 11       | pobj     | sand      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning human nouns\n",
    "Code adapted from the classification work in the [Wild Animal Stories notebook](http://localhost:8888/lab/tree/workspace%2Fwildanimalstories%2Fexperiments.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> reading config files...\n",
      ">> streaming as tsv: /Users/ryan/DH/lit/corpus/chadwyck_poetry/corpus-metadata.ChadwyckPoetry.txt\n",
      "   done [3.3 seconds]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'Z200427100.xml', '1850-1874'),\n",
       " (u'Z200358033.xml', '1975-1999'),\n",
       " (u'Z400369280.xml', '1900-1924'),\n",
       " (u'Z300173395.xml', '1850-1874'),\n",
       " (u'Z200137391.xml', '1850-1874')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lit\n",
    "CP=lit.load_corpus('ChadwyckPoetry')\n",
    "CPgroups = CP.new_grouping()\n",
    "CPgroups.group_by_author_at_30(yearbin=25)\n",
    "CPgroups.prune_groups(min_group=1600,max_group=2000,min_len=10)\n",
    "fn2group=dict((k.split('/')[-1]+'.xml',v) for k,v in CPgroups.textid2group.items())\n",
    "fn2group.items()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit import tools\n",
    "import os\n",
    "def transform_booknlp_like_data(fn='data_booknlp/data.booknlp_like_data.chadwyck_poetry.txt.gz',\n",
    "                                odir='data_booknlp/data_by_quarter_century/'):\n",
    "    \"\"\"\n",
    "    save booknlp-like data in separate files by group\n",
    "    \"\"\"\n",
    "    if not os.path.exists(odir): os.makedirs(odir)\n",
    "    group2f={}\n",
    "    header=None\n",
    "    for dx in tools.readgen(fn):\n",
    "        if not header: header=sorted(list(dx.keys()))\n",
    "        group=fn2group.get(dx['fn'])\n",
    "        if not group: continue\n",
    "        dx['group']=group\n",
    "        ofn=os.path.join(odir,group+'.txt')\n",
    "        import codecs\n",
    "        if not group in group2f:\n",
    "            f=group2f[group]=codecs.open(ofn,'w',encoding='utf-8')\n",
    "            f.write('\\t'.join(h for h in header) + '\\n')\n",
    "        f=group2f[group]\n",
    "        f.write('\\t'.join(dx.get(h,'') for h in header) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform_booknlp_like_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS_WANTED = ['VG.Human','VG.Object','VG.Animal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> streaming as tsv: /Users/ryan/DH/Dissertation/abstraction/words/data.fields.txt\n",
      "   done [0.1 seconds]\n"
     ]
    }
   ],
   "source": [
    "from lit.tools.freqs import get_fields\n",
    "fields=get_fields()\n",
    "#fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'peacock', 'VG.Animal'),\n",
       " (u'coach', 'VG.Human'),\n",
       " (u'liar', 'VG.Human'),\n",
       " (u'rabbit', 'VG.Animal'),\n",
       " (u'corps', 'VG.Human'),\n",
       " (u'fox', 'VG.Animal'),\n",
       " (u'bull', 'VG.Animal'),\n",
       " (u'dollar', 'VG.Object'),\n",
       " (u'commoner', 'VG.Human'),\n",
       " (u'obstruction', 'VG.Object'),\n",
       " (u'manager', 'VG.Human'),\n",
       " (u'pervert', 'VG.Human'),\n",
       " (u'gang', 'VG.Human'),\n",
       " (u'zinc', 'VG.Object'),\n",
       " (u'skin', 'VG.Object'),\n",
       " (u'aristocrat', 'VG.Human'),\n",
       " (u'chair', 'VG.Object'),\n",
       " (u'captain', 'VG.Human'),\n",
       " (u'milk', 'VG.Object'),\n",
       " (u'equipment', 'VG.Object'),\n",
       " (u'voter', 'VG.Human'),\n",
       " (u'grape', 'VG.Object'),\n",
       " (u'buddy', 'VG.Human'),\n",
       " (u'pioneer', 'VG.Human'),\n",
       " (u'gymnast', 'VG.Human')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2field={}\n",
    "for field in FIELDS_WANTED:\n",
    "    for word in fields.get(field,[]):\n",
    "        word2field[word]=field\n",
    "word2field.items()[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = {\n",
    "    'poss':'Possessive',\n",
    "    'nsubj':'Subject',\n",
    "    'dobj':'Object',\n",
    "    'amod':'Modifier'\n",
    "}\n",
    "\n",
    "REL_WORD = True  # True for rel_word ('dobj_roads') or False for word (just 'roads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within groups, start classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_group_file(fn='data_booknlp/data_by_quarter_century/1600-1624.txt',only_rels=rels,only_fields=True):\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv(fn,sep='\\t',encoding='utf-8',quoting=3,error_bad_lines=False)\n",
    "    \n",
    "    if only_rels: df=df.loc[df.rel.isin(rels)]\n",
    "    df['field']=[word2field.get(w,'') for w in df.word]\n",
    "    if only_fields: df=df.loc[df.field!='']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>head</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>rel</th>\n",
       "      <th>word</th>\n",
       "      <th>field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Z200410536.xml</td>\n",
       "      <td>lift</td>\n",
       "      <td>17</td>\n",
       "      <td>poss</td>\n",
       "      <td>hand</td>\n",
       "      <td>VG.Object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Z200410536.xml</td>\n",
       "      <td>acts</td>\n",
       "      <td>17</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>hand</td>\n",
       "      <td>VG.Object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Z300448748.xml</td>\n",
       "      <td>subdues</td>\n",
       "      <td>2</td>\n",
       "      <td>dobj</td>\n",
       "      <td>people</td>\n",
       "      <td>VG.Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Z300448748.xml</td>\n",
       "      <td>is</td>\n",
       "      <td>13</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>hand</td>\n",
       "      <td>VG.Object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Z300448748.xml</td>\n",
       "      <td>set</td>\n",
       "      <td>17</td>\n",
       "      <td>dobj</td>\n",
       "      <td>servant</td>\n",
       "      <td>VG.Human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 fn     head  num_sent    rel     word      field\n",
       "25   Z200410536.xml     lift        17   poss     hand  VG.Object\n",
       "27   Z200410536.xml     acts        17  nsubj     hand  VG.Object\n",
       "94   Z300448748.xml  subdues         2   dobj   people   VG.Human\n",
       "121  Z300448748.xml       is        13  nsubj     hand  VG.Object\n",
       "130  Z300448748.xml      set        17   dobj  servant   VG.Human"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = open_group_file()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'poss'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i,row=df.iterrows().next()\n",
    "row['rel']\n",
    "#list(df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_crosstabs(df,lim_cols=2000,row_sum_min=10,syntax=False,rel_word=REL_WORD):\n",
    "    if rel_word:\n",
    "        df['rel_head']=[unicode(row.get('rel',''))+'_'+unicode(row.get('head','')) for row in df.to_dict('records')]\n",
    "        dfc=pd.crosstab(df['word'],df['rel_head'])\n",
    "    else:\n",
    "        left=pd.crosstab(df['word'], df['head'])\n",
    "        right=pd.crosstab(df['word'], df['rel'])\n",
    "        dfc=left.join(right,rsuffix='_rel')\n",
    "\n",
    "    dfc=dfc.loc[dfc.sum(axis=1)>row_sum_min]\n",
    "    cols=list(dfc.sum(axis=0).nlargest(lim_cols).index)\n",
    "    dfc=dfc[cols]\n",
    "    ## add field\n",
    "    dfc['_field']=[word2field.get(w,'') for w in dfc.index]\n",
    "    return dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rel_head</th>\n",
       "      <th>nsubj_is</th>\n",
       "      <th>nsubj_was</th>\n",
       "      <th>nsubj_be</th>\n",
       "      <th>dobj_have</th>\n",
       "      <th>dobj_give</th>\n",
       "      <th>dobj_make</th>\n",
       "      <th>dobj_had</th>\n",
       "      <th>dobj_take</th>\n",
       "      <th>dobj_see</th>\n",
       "      <th>nsubj_did</th>\n",
       "      <th>...</th>\n",
       "      <th>nsubj_chased</th>\n",
       "      <th>nsubj_check</th>\n",
       "      <th>nsubj_cleanse</th>\n",
       "      <th>nsubj_clown</th>\n",
       "      <th>nsubj_cold</th>\n",
       "      <th>nsubj_command</th>\n",
       "      <th>nsubj_commands</th>\n",
       "      <th>nsubj_commend</th>\n",
       "      <th>nsubj_commends</th>\n",
       "      <th>_field</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acquaintance</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VG.Human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anchor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VG.Object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arm</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VG.Object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrow</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VG.Object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VG.Object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "rel_head      nsubj_is  nsubj_was  nsubj_be  dobj_have  dobj_give  dobj_make  \\\n",
       "word                                                                           \n",
       "acquaintance         0          0         2          2          0          0   \n",
       "anchor               0          0         0          0          0          0   \n",
       "arm                  3          1         2          0          0          2   \n",
       "arrow                0          0         0          0          0          0   \n",
       "art                 23          8        10          9          3          1   \n",
       "\n",
       "rel_head      dobj_had  dobj_take  dobj_see  nsubj_did    ...      \\\n",
       "word                                                      ...       \n",
       "acquaintance         0          5         0          0    ...       \n",
       "anchor               0          0         0          0    ...       \n",
       "arm                  0          0         0          2    ...       \n",
       "arrow                0          0         0          1    ...       \n",
       "art                  6          1         1          4    ...       \n",
       "\n",
       "rel_head      nsubj_chased  nsubj_check  nsubj_cleanse  nsubj_clown  \\\n",
       "word                                                                  \n",
       "acquaintance             0            0              0            0   \n",
       "anchor                   0            0              0            0   \n",
       "arm                      0            0              0            0   \n",
       "arrow                    0            0              0            0   \n",
       "art                      0            0              0            0   \n",
       "\n",
       "rel_head      nsubj_cold  nsubj_command  nsubj_commands  nsubj_commend  \\\n",
       "word                                                                     \n",
       "acquaintance           0              0               0              0   \n",
       "anchor                 0              0               0              0   \n",
       "arm                    0              0               0              0   \n",
       "arrow                  0              0               0              0   \n",
       "art                    0              0               0              0   \n",
       "\n",
       "rel_head      nsubj_commends     _field  \n",
       "word                                     \n",
       "acquaintance               0   VG.Human  \n",
       "anchor                     0  VG.Object  \n",
       "arm                        0  VG.Object  \n",
       "arrow                      0  VG.Object  \n",
       "art                        0  VG.Object  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc=make_crosstabs(df)\n",
    "dfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "def classify(X,y):\n",
    "    loo=LeaveOneOut()\n",
    "    correct=[]\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        clf = LogisticRegression(C=0.001)\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        clf.fit(X_train,y_train)\n",
    "        predictions=clf.predict(X_test)\n",
    "        correct+=[int(y_test[0]==predictions[0])]\n",
    "    return np.mean(correct)\n",
    "\n",
    "from scipy.stats import zscore\n",
    "def do_classification(df,target='_field',numruns=30,numsample=50,replace=False):\n",
    "    target_types=set(list(df[target]))\n",
    "    \n",
    "    objects=[]\n",
    "    for tt1 in target_types:\n",
    "        for tt2 in target_types:\n",
    "            if tt2<=tt1: continue\n",
    "            for nr in range(numruns):\n",
    "                objects+=[(tt1,tt2,nr)]\n",
    "    \n",
    "    import random\n",
    "    random.shuffle(objects)\n",
    "    numobj=len(objects)\n",
    "    for i,(tt1,tt2,nr) in enumerate(objects):\n",
    "        print i,numobj,tt1,tt2,nr,\n",
    "        dfs=[df.loc[df[target]==tt] for tt in [tt1,tt2]]\n",
    "        lens=[len(_df.index) for _df in dfs]\n",
    "        minlen=min(lens)\n",
    "        print lens,minlen,\n",
    "        ns=numsample if numsample else minlen\n",
    "        print ns,\n",
    "        try:\n",
    "            dfs_sample=[_df.sample(ns,replace=replace) for _df in dfs]\n",
    "        except ValueError:\n",
    "            print \"!!\"\n",
    "            continue\n",
    "        \n",
    "        ndf=pd.concat(dfs_sample)\n",
    "        Xdf=ndf.select_dtypes('number').apply(zscore).dropna(1)\n",
    "        y=np.array([word2field[w] for w in Xdf.index])\n",
    "        X=Xdf.values\n",
    "        \n",
    "        acc=classify(X,y)\n",
    "        print acc\n",
    "        odx={'class1':tt1,'class2':tt2,'accuracy':acc,'numruns':numruns,'numrun':nr,'numsample':numsample}\n",
    "        #print odx\n",
    "        yield odx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_group_file(fn='data_booknlp/data_by_quarter_century/1600-1624.txt',\n",
    "                        ofn='data_booknlp/data_by_quarter_century_modeled/1600-1624.txt',\n",
    "                        only_rels=rels,only_fields=True,\n",
    "                        numruns=30,lim_cols=2000):\n",
    "    df=open_group_file(fn=fn,only_rels=only_rels,only_fields=only_fields)\n",
    "    df_tabs=make_crosstabs(df,lim_cols=lim_cols)\n",
    "    for odx in do_classification(df_tabs,numruns=numruns):\n",
    "        odx['fn']=os.path.basename(fn)\n",
    "        odx['period_int']=fn.split('-')[0]\n",
    "        yield odx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 90 VG.Human VG.Object 11 [84, 274] 84 50 0.72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.72,\n",
       " 'class1': 'VG.Human',\n",
       " 'class2': 'VG.Object',\n",
       " 'fn': '1600-1624.txt',\n",
       " 'numrun': 11,\n",
       " 'numruns': 30,\n",
       " 'numsample': 50,\n",
       " 'period_int': 'data_booknlp/data_by_quarter_century/1600'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_group_file().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_classify_group_file(fn):\n",
    "    return list(classify_group_file(fn))\n",
    "\n",
    "def classify_all(idir='data_booknlp/data_by_quarter_century',\n",
    "                 odir='data_booknlp/data_by_quarter_century_modeled',lim_cols=2000):\n",
    "    import multiprocessing as mp\n",
    "    pool=mp.Pool()\n",
    "    filenames = [os.path.join(idir,fn) for fn in os.listdir(idir) if fn.endswith('.txt')]\n",
    "    for old in pool.imap_unordered(do_classify_group_file, filenames):\n",
    "        for odx in old:\n",
    "            yield odx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools.writegen('data_booknlp/data.classification_results.txt', classify_all)\n",
    "# last run, V2 (with normalization), 2/4/19 15:09\n",
    "\n",
    "#tools.writegen('data_booknlp/data.classification_results.txt', classify_all)\n",
    "# last run, V3 (with rel_word), 2/4/19 16:15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "* Accuracy looks ok? These are all binary classification problems. Thirty times (numruns) per quarter-century, predicting between 50 words (numsamples) of class1 and 50 words of class2, *without* standardization:\n",
    "\n",
    "<center><img src=\"images/Accuracy for predicting humananimalobject, 1600-2000.png\" width=\"500\"></center>\n",
    "\n",
    "* Accuracy gets worse when we turn on standardization (Z-score) for features. Why?\n",
    "\n",
    "<center><img src=\"images/Accuracy for predicting humananimalobject, 1600-2000.V2 with standardization.png\" width=\"500\"></center>\n",
    "\n",
    "* Accuracy marginally better when \"rel_head\" (eg nsubj_knows) is used and not just \"head\" (eg knows). (Z-scores used)\n",
    "\n",
    "<center><img src=\"images/Accuracy for predicting humananimalobject, 1600-2000.V3 with rel_word.png\" width=\"500\"></center>\n",
    "\n",
    "* Median accuracy at the end of the day: **75%**, Human-vs-Object. Is that good enough to justify the next step?\n",
    "\n",
    "<center><img src=\"images/Median accuracy rates per classification task (across all runs of all periods).png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step: estimating humanness\n",
    "\n",
    "Here we applying this machine-learnt model (separating human and objects) to estimate the 'humanness' of all other words in the data. We're not concerned with whether these estimations are “right,” per se, but more in the pattern of their wrongness: the word “nature” is not a person, but is there a history to its person-likeness? Is “nature” ever... dare I say... anthropomorphic: human-*like*, according to the model?\n",
    "\n",
    "\n",
    "### Saving cross-tabs (2000MFW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save crosstabs\n",
    "def save_crosstabs(fn='data_booknlp/data_by_quarter_century/1600-1624.txt',\n",
    "                          odir='data_booknlp/data_by_quarter_century__crosstabs',\n",
    "                    only_rels=rels,field1='VG.Human',field2='VG.Object',\n",
    "                    row_sum_min=10,lim_cols=2000):\n",
    "    df=pd.read_csv(fn,sep='\\t',encoding='utf-8',quoting=3,error_bad_lines=False)\n",
    "    \n",
    "    if only_rels: df=df.loc[df.rel.isin(rels)]\n",
    "    df['field']=[word2field.get(w,'') for w in df.word]\n",
    "    #df=df.loc[df.field.isin({field1,field2})]\n",
    "    \n",
    "    ## make crosstabs\n",
    "    print '>> crosstabbing head counts',tools.now()\n",
    "    counts_head=pd.crosstab(df['word'], df['head'])\n",
    "    print '>> crosstabbing rel counts',tools.now()\n",
    "    counts_rel=pd.crosstab(df['word'], df['rel'])\n",
    "    print '>> joining tables',tools.now()\n",
    "    df_tabs=counts_head.join(counts_rel,rsuffix='_rel')\n",
    "    print '>> filtering by row_sum_min',tools.now()\n",
    "    if row_sum_min: df_tabs=df_tabs.loc[df_tabs.sum(axis=1)>row_sum_min]\n",
    "    print '>> filtering by lim_cols',tools.now()\n",
    "    if lim_cols:\n",
    "        cols=list(df_tabs.sum(axis=0).nlargest(lim_cols).index)\n",
    "        df_tabs=df_tabs[cols]\n",
    "    ## add field\n",
    "    print '>> adding new column',tools.now()\n",
    "    df_tabs['_field']=[word2field.get(w,'') for w in df_tabs.index]\n",
    "    \n",
    "    if not os.path.exists(odir): os.makedirs(odir)\n",
    "    ofnfn=os.path.join(odir, os.path.basename(fn))\n",
    "    print '>> saving',tools.now()\n",
    "    df_tabs.to_csv(ofnfn,sep='\\t',encoding='utf-8')\n",
    "    print '>> saved:',ofnfn,tools.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_crosstabs(idir='data_booknlp/data_by_quarter_century/'):\n",
    "    ifiles = [os.path.join(idir,ifn) for ifn in os.listdir(idir) if ifn.endswith('.txt')]\n",
    "    tools.crunch(ifiles, save_crosstabs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_all_crosstabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "def classify_from_crosstabs(fn='data_booknlp/data_by_quarter_century__crosstabs/1900-1924.txt',\n",
    "                    odir='data_booknlp/data_by_quarter_century__model_results/',\n",
    "                    field1='VG.Human',field2='VG.Object',target='_field',\n",
    "                    lim_cols=1000):\n",
    "    df_tabs=pd.read_csv(fn,sep='\\t',encoding='utf-8',quoting=3,error_bad_lines=False).fillna('').set_index('word')\n",
    "    \n",
    "    word2field=dict(zip(df_tabs.index,df_tabs[target]))\n",
    "    #df_tabs.drop(target,inplace)\n",
    "    \n",
    "    #if lim_cols:\n",
    "    #    cols=df_tabs.select_dtypes('number').sum(axis=0).nlargest(lim_cols).index\n",
    "    #    df_tabs=df_tabs[list(cols) + ['_field']]\n",
    "    \n",
    "    ### make test and training sets\n",
    "    df_train = df_tabs.loc[df_tabs._field.isin({field1,field2})]\n",
    "    df_test = df_tabs.loc[~df_tabs._field.isin({field1,field2})]\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(C=0.001)\n",
    "    \n",
    "    # Fit\n",
    "    X_train=df_train.select_dtypes('number').apply(zscore).dropna(1) #.values\n",
    "    y_train=[word2field[w] for w in X_train.index]\n",
    "    clf.fit(X_train.values,y_train)\n",
    "    \n",
    "    # Save model results\n",
    "    if not os.path.exists(odir): os.makedirs(odir)\n",
    "    ofnfn=os.path.join(odir,os.path.basename(fn))\n",
    "    \n",
    "    df_feats = pd.DataFrame(clf.coef_).T\n",
    "    df_feats.columns = ['coeff']\n",
    "    df_feats['feat']=X_train.columns\n",
    "    #df_feats.set_index('feat',inplace=True)\n",
    "    df_feats.to_csv(ofnfn,sep='\\t',encoding='utf-8')\n",
    "    print '>> saved:',ofnfn\n",
    "    \n",
    "    # Predict\n",
    "    X = df_tabs.select_dtypes('number').apply(zscore).dropna(1)\n",
    "    X = X[X_train.columns]\n",
    "    predictions=clf.predict_proba(X.values)\n",
    "    n_dim = len(predictions[0])\n",
    "    header=[('ProbClass%s' % (i+1)) for i in range(n_dim)]\n",
    "    df_result=pd.DataFrame(predictions, columns=header)\n",
    "    df_result['word']=df_tabs.index\n",
    "    #df_result=df_result.set_index('word')\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify_from_crosstabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_all_from_crosstabs(idir='data_booknlp/data_by_quarter_century__crosstabs'):\n",
    "    paths = [os.path.join(idir,fn) for fn in sorted(os.listdir(idir)) if fn.endswith('.txt')]\n",
    "    for path in paths:\n",
    "        print '>>',path\n",
    "        df_result=classify_from_crosstabs(path)\n",
    "        ld_result = df_result.to_dict('records')\n",
    "        for dx in ld_result:\n",
    "            dx['fn']=os.path.basename(path)\n",
    "            dx['period']=dx['fn'].split('-')[0]\n",
    "            yield dx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify_all_from_crosstabs().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools.writegen('data_booknlp/data.classification_probabilities_by_word_by_period.txt', classify_all_from_crosstabs)\n",
    "# last run, V2 (with standardization), 2/4/2019 in the morning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "* Meaningfulness of results seems to have improved with the turn to standardization. Here are the results without standardization:\n",
    "\n",
    "<center><img src=\"images/Probability of being a human V1.png\" width=\"500\"></center>\n",
    "\n",
    "* Here are the results with standardization:\n",
    "\n",
    "<center><img src=\"images/Probability of being a human V2.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do (2/4)\n",
    "\n",
    "* Make a better worddb! I want less a million vectors, and more semantic fields (column \"VG\" should have \"Human\", \"Object\", etc). This way I can remove the VG.Human/VG.Object's from the results. (Or I could switch \"X\" to \"X_test\" above.)\n",
    "* Investigate feature loadings. What predicts humanness? Maybe switch features to \"rel_word\". More meaningful that way.\n",
    "* Can we use these features to classify moments of personification 'in real time', i.e. in the text?\n",
    "* ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
