{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics of personhood\n",
    "\n",
    "Classification experiment for nouns: are they human or not? Can a noun be thought of as human by its grammar, by its modifiers and possessives and verbs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gather seed lists of human/nonhuman words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BYU function\n",
    "import pytxt\n",
    "def get_byu_dd(fn='/Users/ryan/DH/TOOLS/words/byu/worddb.byu.txt'):\n",
    "    return pytxt.ld2dd(pytxt.read_ld(fn),'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_byu_dd()['summon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Semantic lists, pruned from Harvard General Inquirer\n",
    "# originally for Wild Animal Stories project, pruned again (less strictly) for human nouns\n",
    "# downloaded from v2, http://localhost:8888/lab/tree/workspace%2Fwildanimalstories%2Fparsing.ipynb\n",
    "###\n",
    "\n",
    "def get_field_words(fnfn='wordlists/Word Lists.xlsx',only_fields={'Human','Animal','Object'},remove_col='notstrict',byu_pos='nn1'):\n",
    "    pos_d=None\n",
    "    if byu_pos:\n",
    "        pos_d=dict( (w,wd['pos']) for w,wd in get_byu_dd().items() )\n",
    "    \n",
    "    import pytxt\n",
    "    from collections import defaultdict,Counter\n",
    "    field2words=defaultdict(set)\n",
    "    field2count=Counter()\n",
    "    field2count_removed=Counter()\n",
    "    for d in pytxt.read_ld(fnfn):\n",
    "        field=d['field'].replace('HGI.','').replace('Person','Human')\n",
    "        if only_fields and not field in only_fields:continue\n",
    "        word=d['word'].lower().strip()\n",
    "        field2count[field]+=1\n",
    "        toremove=d[remove_col].strip()\n",
    "        if toremove and toremove.lower()=='y': continue\n",
    "        if not word: continue\n",
    "            \n",
    "        if byu_pos and pos_d and pos_d.get(word)!=byu_pos: continue\n",
    "            \n",
    "        field2count_removed[field]+=1\n",
    "        field2words[field]|={d['word']}\n",
    "    #statld=[{'fieldname':name, 'num_words':count,'num_words_after_pruning':field2count_removed[name], 'words':' '.join(field2words[name])} for name,count in field2count.items()]\n",
    "    #print pytxt.tabify(statld)\n",
    "    return field2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_fields():\n",
    "    D={}\n",
    "    for k,v in get_field_words(remove_col='strict',byu_pos='nn1').items():\n",
    "        D[k.strip()]=v\n",
    "    D['Human (V2)']=get_field_words(remove_col='notstrict',byu_pos='nn1')['Human']\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#field2words=get_all_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# for fld in sorted(field2words):\n",
    "#     print fld,'-->',random.sample(field2words[fld],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Storing these in the big list of semantic fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added above function get_field_words() to the [Semantic Fields Notebook](http://localhost:8888/lab/tree/Dissertation%2Fabstraction%2Fwords%2Fsemantic_fields.ipynb), pointing to the same location (wordlists/Word Lists.xlsx).\n",
    "\n",
    "Fields Animal, Human, and Object are stored with the prefix \"VG\": VG.Animal, VG.Human, VG.Object.\n",
    "\n",
    "This saves a new version of */Users/ryan/DH/Dissertation/abstraction/words/data.fields.txt* and */Users/ryan/DH/Dissertation/abstraction/words/data.field_words.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now available at\n",
    "# from lit.tools.freqs import get_fields\n",
    "# fields=get_fields() # which gets /Users/ryan/DH/Dissertation/abstraction/words/data.fields.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fld in ['Human','Animal','Object']:\n",
    "#     print fld,'-->',random.sample(fields['VG.'+fld],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculating humanness vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changed word2vec.py's abstract_vectors():\n",
    "\n",
    "    def abstract_vectors(self,only_major=True,include_social=False):\n",
    "            model = self.gensim\n",
    "            vd={} # vector dictionary\n",
    "\n",
    "            from lit.freqs import get_fields\n",
    "            fields = get_fields()\n",
    "\n",
    "            vd['Complex Substance (Locke) <> Mixed Modes (Locke)'] = self.centroid(fields['Locke.MixedMode']) - self.centroid(fields['Locke.ComplexIdeaOfSubstance'])\n",
    "            vd['Concrete (HGI) <> Abstract (HGI)'] = self.centroid(fields['HGI.Abstract']) - self.centroid(fields['HGI.Concrete'])\n",
    "            vd['Human (VG)'] = self.centroid(fields['VG.Human'])\n",
    "            vd['Object (VG) <> Human (VG)'] = self.centroid(fields['VG.Human']) - self.centroid(fields['VG.Object'])\n",
    "            vd['Animal (VG) <> Human (VG)'] = self.centroid(fields['VG.Human']) - self.centroid(fields['VG.Animal'])\n",
    "            vd['Vice (HGI) <> Virtue (HGI)']=self.centroid(fields['HGI.Moral.Virtue']) - self.centroid(pytxt.fields['HGI.Moral.Vice'])\n",
    "\n",
    "            return vd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Examining vectors in Chadwyck Poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lit\n",
    "chadwyck_poetry = lit.load_corpus('ChadwyckPoetry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_w2v = chadwyck_poetry.word2vec_by_period()\n",
    "cp_w2v.models[0].fnfn, cp_w2v.models[-1].fnfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save partial models?\n",
    "#maxrank=10000\n",
    "#cp_w2v.limit_vocab_and_save('/Users/ryan/DH/corpora/chadwyck_poetry/word2vec_models_partial_10K', n=maxrank,fpm_cutoff=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4A. model_words(): Historicizing the humanness vector for words\n",
    "V1 (2/2/2019): with full vocab models\n",
    "V2 (2/3/2019): with vocab-limited models\n",
    "\n",
    "**Update (2/3/19):** Vocab-limited models made no difference on below results. Data files were overwritten (woops), but visualizations are from full vocab model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp_w2v.model_words(abstract_vectors=True,odir='data_word2vec/chadwyck_poetry')\n",
    "# last run: 2/1/2019 22:06\n",
    "\n",
    "# with limited models:\n",
    "#cp_w2v.model_words(abstract_vectors=True,odir='data_word2vec/chadwyck_poetry_models_partial_10K')\n",
    "# last run: 2/3/2019 with V2, vocab limited models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp_w2v.consolidate_model_words(idir='data_word2vec/chadwyck_poetry/')\n",
    "# last run: 2/1/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary output of consolidate_model_words() is analyzed in [this Tableau file](data_word2vec/data.word2vec.consolidated.words.ChadwyckPoetry.by_period.Noneyears.twb).\n",
    "\n",
    "#### Some of the figures and results\n",
    "\n",
    "##### How humanness vectors relate to each other\n",
    "\n",
    "* The standard humanness vector is difficult to understand, with things like \"ax\" and \"sop\" and \"sling\" close to the humanness vector. Here it is contrasted with (for me) more legible vector V(Object-Human) (R^2=0.13):\n",
    "\n",
    "<center><img src=\"figures/HumanObject vs Human.png\" width=500></center>\n",
    "\n",
    "* The V(Animal-Human) vector correlates even less with V(Human) (R^2=0.05):\n",
    "\n",
    "<center><img src=\"figures/HumanAnimal vs Human.png\" width=500></center>\n",
    "\n",
    "* By contrast, the V(Animal-Human) correlates much better with V(Object-Human), although interesting differences remain (R^2=0.67):\n",
    "\n",
    "<center><img src=\"figures/HumanObjectAnimal.png\" width=500></center>\n",
    "\n",
    "##### How humanness relates to abstractness\n",
    "\n",
    "* The Human-Object vs V(Conc-Abs)[Locke] vector doesn't show much correlation (R^2=0.25):\n",
    "\n",
    "<center><img src=\"figures/ConcAbsHumanObject.png\" width=500></center>\n",
    "\n",
    "* The Human-Object vs V(Conc-Abs)[HGI] vector shows more correlation (R^2=0.55):\n",
    "\n",
    "<center><img src=\"figures/ConcAbs (HGI) vs HumanObject.png\" width=500></center>\n",
    "\n",
    "##### How humanness relates to virtue and vice\n",
    "\n",
    "* The Vice-Virtue vector correlates with the **Object**-Human vector (R^2=0.24):\n",
    "\n",
    "<center><img src=\"figures/HumanObjectViceVirtue.png\" width=500></center>\n",
    "\n",
    "* The Vice-Virtue vector correlates with the Animal-Human vector (R^2=0.45):\n",
    "\n",
    "<center><img src=\"figures/HumanAnimalViceVirtue.png\" width=500></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4B. Correlate historical humanness vector results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with results of Step 4A in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data_word2vec/data.word2vec.consolidated.words.ChadwyckPoetry.by_period.Noneyears.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_period = 1600\n",
    "max_period = 1900\n",
    "min_count = 100\n",
    "df['period_int']=[int(x.split('-')[0]) for x in df.period]\n",
    "df=df.loc[df.period_int>=min_period]\n",
    "df=df.loc[df.period_int<max_period]\n",
    "df=df.loc[df.model_count>=min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['period_word']=[(x,y) for x,y in zip(df.period,df.word)]\n",
    "groups=df.groupby('period_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#df.groupby('period')\n",
    "meandf=df.groupby('period_word').mean()\n",
    "#meandf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meandf['period'],meandf['word'] = zip(*meandf.index)\n",
    "#pivotdf=meandf.pivot(index='word',columns='period',values='Human (VG)')\n",
    "pivotdf=meandf.pivot_table(index='word',columns='period',values='Object (VG) <> Human (VG)', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(pivotdf.dropna().index)\n",
    "datadf=pivotdf.dropna()\n",
    "\n",
    "# standardize\n",
    "from scipy.stats import zscore\n",
    "datadf=datadf.apply(zscore)\n",
    "datadf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://localhost:8888/lab/tree/workspace%2Fsyntax%2Fcorrelations.ipynb\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "def newpolyfit(X,Y):\n",
    "    newdf=pd.DataFrame({'X':X, 'Y':Y})\n",
    "    results = smf.ols('Y ~ X + I(X**2)', data=newdf).fit()\n",
    "    return results.rsquared, results.f_pvalue\n",
    "\n",
    "def regressions(df):\n",
    "    word2rp={}\n",
    "    df=df.T\n",
    "    for word in df.columns:\n",
    "        Y=list(df[word])\n",
    "        X=list(range(len(Y)))\n",
    "        word2rp[word]=newpolyfit(X,Y)\n",
    "    return word2rp\n",
    "\n",
    "#%load_ext rpy2.ipython\n",
    "\n",
    "def dist(df):\n",
    "    from scipy.spatial.distance import squareform, pdist\n",
    "    distmatrix=pdist(df,metric='correlation')\n",
    "    return 1-pd.DataFrame(squareform(distmatrix), columns=df.index, index=df.index)\n",
    "\n",
    "def kmeans(datadf,n_kmeans=5):\n",
    "    df_dist=dist(datadf)\n",
    "    m_dist=df_dist.values\n",
    "    from sklearn.cluster import KMeans\n",
    "    model_kclust = KMeans(n_clusters=n_kmeans)\n",
    "    model_kclust.fit(m_dist)\n",
    "    labels = model_kclust.labels_\n",
    "    word2label = dict(zip(datadf.index, labels))\n",
    "    return word2label\n",
    "\n",
    "def corr_with_cluster(df,word2cluster):\n",
    "    # cluster2words\n",
    "    from collections import defaultdict\n",
    "    cluster2words=defaultdict(list)\n",
    "    for w,c in word2cluster.items():\n",
    "        cluster2words[c]+=[w]\n",
    "    \n",
    "    # get avg per cluster\n",
    "    cluster_avg={}\n",
    "    for clust,words in cluster2words.items():\n",
    "        cluster_avg[clust]=list(df.loc[words].median(axis=0))\n",
    "    \n",
    "    # corr with each word\n",
    "    from scipy.stats.stats import pearsonr\n",
    "    word2clustcorr={}\n",
    "    for word,clust in word2cluster.items():\n",
    "        word_avgs=list(df.loc[word])\n",
    "        clust_avgs=cluster_avg[clust]\n",
    "        word2clustcorr[word]=pearsonr(word_avgs,clust_avgs)\n",
    "    return word2clustcorr\n",
    "\n",
    "def tsne(datadf,n_components=2):\n",
    "    df_dist=dist(datadf)\n",
    "    m_dist=df_dist.values\n",
    "    from sklearn.manifold import TSNE\n",
    "    model = TSNE(n_components=n_components, random_state=0)\n",
    "    fit = model.fit_transform(m_dist)\n",
    "    from collections import defaultdict\n",
    "    newcols=defaultdict(list)\n",
    "    for i,word in enumerate(datadf.index):\n",
    "        for ii,xx in enumerate(fit[i]):\n",
    "            newcols['tsne_V'+str(ii+1)] += [xx]\n",
    "    for k,v in newcols.items(): datadf[k]=v\n",
    "    return datadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datadf.loc[['virtue','vice']].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2cluster=kmeans(datadf,n_kmeans=3)\n",
    "word2clustcorr=corr_with_cluster(datadf,word2cluster)\n",
    "word2rp=regressions(datadf)\n",
    "datadf=tsne(datadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = datadf.copy()\n",
    "df_out['kmeans_cluster'] = [word2cluster.get(w,'') for w in df_out.index]\n",
    "df_out['polyfit_r2'] = [word2rp[w][0] if w in word2rp else '' for w in df_out.index]\n",
    "df_out['polyfit_p'] = [word2rp[w][1] if w in word2rp else '' for w in df_out.index]\n",
    "df_out['corr_w_clust_r'] = [word2clustcorr[w][0] if w in word2clustcorr else '' for w in df_out.index]\n",
    "df_out['corr_w_clust_p'] = [word2clustcorr[w][1] if w in word2clustcorr else '' for w in df_out.index]\n",
    "df_out['word']=df_out.index\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('data_word2vec/data.word2vec.consolidated.words.ChadwyckPoetry.by_period.Noneyears.CORRELATIONS.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some results are:\n",
    "    \n",
    "* K-means with 3 clusters divides history into three movements:\n",
    "\n",
    "<center><img src=\"figures/Cluster Aggregate Trends.png\" width=400></center>\n",
    "    \n",
    "* Here are the individual words broken down, in 1K word buckets by rank (BYU):\n",
    "\n",
    "<center>\n",
    "    <img src=\"figures/Pages by Rank - 0K-1K.png\" width=800>\n",
    "    <br/>\n",
    "    <img src=\"figures/Pages by Rank - 1K-2K.png\" width=800>\n",
    "    <br/>\n",
    "    <img src=\"figures/Pages by Rank - 2K-3K.png\" width=800>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few words to notice\n",
    "\n",
    "* The word \"fall\" falls on V(Human-Object): turn away from its sense as The (Xtian) Fall?\n",
    "\n",
    "<center><img src=\"figures/vhumanobj-fall.png\" width=400></center>\n",
    "\n",
    "* The word \"world\" rises and falls on V(Human-Object): the social-ization of the world? The public sphere?\n",
    "\n",
    "<center><img src=\"figures/vhumanobj-world.png\" width=400></center>\n",
    "\n",
    "* The word \"nature\" falls and rises on V(Human-Object): I don't know if I trust this... isn't Nature constantly personified in C18?\n",
    "\n",
    "<center><img src=\"figures/vhumanobj-nature.png\" width=400></center>\n",
    "\n",
    "* Domestic objects:\n",
    "\n",
    "<center><img src=\"figures/vhumanobj-domesticobjs.png\" width=400></center>\n",
    "\n",
    "* Body parts:\n",
    "\n",
    "<center><img src=\"figures/vhumanobj-bodypart.png\" width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> reading config files...\n"
     ]
    }
   ],
   "source": [
    "from lit import load_corpus\n",
    "CP = load_corpus('ChadwyckPoetry')\n",
    "CPw2v = CP.word2vec_by_period()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = m1600,m1700,m1800,m1900 = CPw2v.period2models['1600-1624'][0], CPw2v.period2models['1700-1724'][0], CPw2v.period2models['1800-1824'][0], CPw2v.period2models['1900-1924'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> streaming as tsv: /Users/ryan/DH/Dissertation/abstraction/words/data.fields.txt\n",
      "   done [0.1 seconds]\n"
     ]
    }
   ],
   "source": [
    "from lit.tools.freqs import get_fields\n",
    "fields=get_fields()\n",
    "#fields['VG.Human']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_similarity(words):\n",
    "    for m in models:\n",
    "        print m.period\n",
    "        m.limit_by_rank(10000)\n",
    "        print ' '.join(x for x,y in m.similar(words,10))\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    for m in models:\n",
    "        print m.name\n",
    "        print ' '.join(x for x,y in m.analogy('man','woman','king'))\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600-1624.run=01.txt.gz\n",
      "princess queen daughter prince empress henry philip iames anne duke\n",
      "\n",
      "1700-1724.run=01.txt.gz\n",
      "goddess queen jupiter sister dow'r priestess prophetess defender earl nero\n",
      "\n",
      "1800-1824.run=01.txt.gz\n",
      "queen prince princess sybil queen's monarch mistress naples pharaoh daughter\n",
      "\n",
      "1900-1924.run=01.txt.gz\n",
      "queen solomon menelaus redivagate bowing porch mycenae rashumba priam cassandra\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600-1624\n",
      "maid girle matron lone iphis helen mother wench acis cuckold\n",
      "\n",
      "1700-1724\n",
      "creature shepherdess orphan hermit nymph's doting zelinda jilt myra cloe\n",
      "\n",
      "1800-1824\n",
      "mother babe creature maid father maiden daughter boy's son nurse\n",
      "\n",
      "1900-1924\n",
      "lover beggar absalom seer thief madman son weeps aunt lad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show_similarity(fields['VG.Human'])\n",
    "show_similarity(['man','woman','girl','boy','child','parent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
